{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[개념정리]\n",
        "==============\n",
        "데이터 전처리\n",
        "\n",
        "데이터 인코딩 - 레이블 인코딩 / 원-핫 인코딩\n",
        "- 레이블 인코딩 : 카테고리 피처를 코드형 숫자 값으로 변환하는 것\n",
        "  - 숫자 값의 경우 크고 작음에 대한 특성이 작용하기 때문에, 일괄적인 숫자값으로 변환되면서 예측 성능이 떨어지는 경우가 발생할 수 있음 -> 선형회귀와 같은 ML 알고리즘에는 적용 x\n",
        "  - LabelEncoder 객체의 classes_ 속성값 : 문자열 값이 어떤 숫자 값으로 인코딩됐는지 파악 가능\n",
        "  - inverse_transform() : 인코딩 값 다시 디코딩\n",
        "- 원-핫 인코딩\n",
        "  - 피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시하는 방식\n",
        "  - 행 형태로 되어있는 피처의 고유 값을 열 형태로 차원을 변환한 뒤, 고유값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0 표시\n",
        "  - get_dummies() : 판다스에서 원-핫 인코딩 쉽게 하는 법 (숫자로 변환 안해도 됨)\n",
        "\n",
        "  주의할 점\n",
        "  1. OneHotEncoder로 변환하기 전에 모든 문자열 값이 숫자형 값으로 반환되어야 함\n",
        "  2. 입력 값으로 2차원 데이터가 필요함\n",
        "\n",
        "피처 스케일링 : 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업\n",
        "- 표준화, 정규화 -> 피처 스케일링\n",
        "- 표준화 : 데이터의 피처 각각이 평균이 0이고, 분산이 1인 가우시안 정규 분포를 가진 값으로 변환하는 것\n",
        "  - xi_new = ( xi - mean(x) ) / stdev(x)\n",
        "- 정규화 : 서로 다른 피처의 크기를 통일하기 위해 크기를 변환해주는 개념\n",
        "  - xi_new = ( xi - min(x) ) / ( max(x) - min(x) )\n",
        "- 사이킷런의 Normarlizer 모듈 : 개별 벡터의 크기를 맞추기 위해 변환하는 것 (개별 벡터를 모든 피처 벡터의 크기로 나눠줌) -> 벡터 정규화\n",
        "  - xi_new = xi / sqrt(xi^2 + yi^2 + zi^2)\n",
        "\n",
        "StandardScalar : 표준화를 쉽게 지원하기 위한 클래스\n",
        "- 개별 피처를 평균이 0이고, 분산이 1인 값으로 변환해줌\n",
        "- RBF 커널을 이용하는 서포트 벡터 머신, 선형회귀, 로지스틱 회귀는 데이터가 가우시안 분포를 가지고 있다고 가정함 -> 표준화를 적용하면 예측 성능이 향상됨\n",
        "\n",
        "MinMaxScaler : 데이터값을 0과 1사이의 범위 값으로 변환 (음수값이 있으면 -1~ 1로 변환)\n",
        "\n",
        "학습 데이터와 테스트 데이터의 스케일링 변환 시 유의점\n",
        "- fit_transform() : fit, transform 한번에 적용\n",
        "- Scaler 객체를 이용해 학습 데이터 세트로 fit()과 transform()을 적용하면 테스트 데이터 세트로는 다시 fit()을 수행하지 않고 학습 데이터 세트로 fit()을 수행한 결과를 이용해 transform() 변환을 적용해야 함\n",
        "- 전체 데이터 세트에 스케일링을 적용한 뒤, 학습과 테스트 데이터 세트로 분리하기 (안되면 위 방법 수행)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "absCNGzm8Ii8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVJe3MYN7fTI"
      },
      "outputs": [],
      "source": [
        "# pg.119 : 레이블 인코딩\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "items=['TV', '냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
        "\n",
        "# LabelEncoder를 객체로 생성한 후, fit()과 transform으로 레이블 인코딩 수행 \n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "labels=encoder.transform(items)\n",
        "print('인코딩 변환값:', labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코딩 클래스:', encoder.classes_)   # LabelEncoder 객체의 classes_ 속성값"
      ],
      "metadata": {
        "id": "g2FapiLm83L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('디코딩 원본값:', encoder.inverse_transform ([4,5,2,0,1,1,3,3]))    # inverse_transform() : 인코딩 값 다시 디코딩"
      ],
      "metadata": {
        "id": "i49WLgiR9Fwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pg.120 : 원-핫 인코딩\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "items=['TV', '냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
        "\n",
        "# 먼저 숫자 값으로 변환을 위해 LabelEncoder로 변환합니다\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "labels = encoder.transform(items)\n",
        "\n",
        "# 2차원 데이터로 변환합니다.\n",
        "labels=labels.reshape(-1,1)\n",
        "\n",
        "# 원-핫 인코딩을 적용합니다. \n",
        "oh_encoder = OneHotEncoder()\n",
        "oh_encoder.fit(labels)\n",
        "oh_labels = oh_encoder.transform(labels)\n",
        "print('원-핫 인코딩 데이터')\n",
        "print(oh_labels.toarray())\n",
        "print('원-핫 인코딩 데이터 차원')\n",
        "print(oh_labels.shape)    # 8개의 레코드와 1개의 칼럼을 가진 원본데이터가 8개~, 6개의~ 데이터로 변환됨"
      ],
      "metadata": {
        "id": "fF0aFgQS9haU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.DataFrame({'items':['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']})\n",
        "\n",
        "pd.get_dummies(df)    # get_dummies() : 판다스에서 원-핫 인코딩 쉽게 하는 법 (숫자로 변환 안해도 됨)"
      ],
      "metadata": {
        "id": "YEzGZxs8_Zke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pg.125 : StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# 붓꽃 데이터 세트를 로딩하고 DataFrame으로 변환한다. \n",
        "iris = load_iris()\n",
        "iris_data=iris.data\n",
        "iris_df=pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
        "\n",
        "print('feature들의 평균 값')\n",
        "print(iris_df.mean())\n",
        "print('\\n feature들의 분산 값')\n",
        "print(iris_df.var())"
      ],
      "metadata": {
        "id": "huMc9iVzAu9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# StandardScaler를 이용해 각 피처를 한번에 표준화해서 변환\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler 객체 생성 \n",
        "scaler = StandardScaler()\n",
        "# StandardScaler로 데이터 세트 변환, fit()과 transform() 호출\n",
        "scaler.fit(iris_df)\n",
        "iris_scaled = scaler.transform(iris_df)\n",
        "\n",
        "# transform()시 스케일 변환된 데이터 세트가 Numpy ndarray로 반환돼 이를 DataFrame으로 변환 \n",
        "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
        "print('feature들의 평균 값')\n",
        "print(iris_df_scaled.mean())\n",
        "print('\\nfeature들의 분산 값')\n",
        "print(iris_df_scaled.var())   # 평균 : 0에 가까움 / 분산 : 1에 가까움"
      ],
      "metadata": {
        "id": "cG0XfLWpF-VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pg.127 : MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# MinMaxScaler 객체 생성\n",
        "scaler = MinMaxScaler()\n",
        "# MinMaxScaler로 데이터 세트 변환. fit()과 transform() 호출.\n",
        "scaler.fit(iris_df)\n",
        "iris_scaled = scaler.transform(iris_df)\n",
        "\n",
        "# transform() 시 스케일 변환된 데이터 세트가 NumPy ndarray로 반환돼 이를 DataFrame으로 변환\n",
        "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
        "print('feature들의 최솟값')\n",
        "print(iris_df_scaled.min())\n",
        "print('feature들의 최댓값')\n",
        "print(iris_df_scaled.max())"
      ],
      "metadata": {
        "id": "8cSbkTrQGxJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pg.128 : 학습 데이터와 테스트 데이터의 스케일링 변환 시 유의점\n",
        "# 테스트 데이터에 fit()을 적용할 때 생기는 문제\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# 학습 데이터는 0부터 10까지, 테스트 데이터는 0부터 5까지 값을 가지는 데이터 세트로 생성\n",
        "# Scaler 클래스의 fit(), transform()은 2차원 이상 데이터만 가능하므로 reshape(-1,1)로 차원 변경\n",
        "train_array = np.arange(0, 11).reshape(-1, 1)\n",
        "test_array = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "# MinMaxScaler 객체의 별도의 feature_range 파라미트 값을 지정하지 않으면 0~1 값으로 변환\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# fit()하게 되면 train_array 데이터의 최솟값이 0, 최댓값이 10으로 설정\n",
        "scaler.fit(train_array)\n",
        "\n",
        "# 1/10 scale로 train_array 데이터 변환함. 원본 10 -> 1 로 변환됨\n",
        "train_scaled = scaler.transform(train_array)\n",
        "\n",
        "print('원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))\n",
        "print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))\n",
        "     "
      ],
      "metadata": {
        "id": "LSuZIVIxH4vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 세트를 변환하는데, fit()을 호출해서 스케일링 기준 정보를 다시 적용한 뒤 transform() 수행\n",
        "\n",
        "# MinMaxScaler에 test_array를 fit()하게 되면 원본 데이터의 최솟값이 0, 최댓값이 5로 설정됨\n",
        "scaler.fit(test_array)\n",
        "\n",
        "# 1/5 Scale로 test_array 데이터를 변환함. 원본 5 -> 1 로 변환\n",
        "test_scaled = scaler.transform(test_array)\n",
        "\n",
        "# test_array의 scale 변환 출력\n",
        "print('원본 test_array 데이터:', np.round(test_array.reshape(-1), 2))\n",
        "print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))"
      ],
      "metadata": {
        "id": "2yUB8XzWQt03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> 위 둘을 통해, 학습 데이터와 테스트 데이터의 스케일링이 맞지 않음을 볼 수 있음"
      ],
      "metadata": {
        "id": "N-JCD2SrR9gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터에 fit()호출 x / 학습 데이터로 fit() 수행한 MinMaxScaler 객체의 transform() 이용해서 데이터 변환\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_array)\n",
        "train_scaled = scaler.transform(train_array)\n",
        "print('원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))\n",
        "print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))\n",
        "\n",
        "# test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 transform()만으로 변환해야 함\n",
        "test_scaled = scaler.transform(test_array)\n",
        "print('\\n원본 test_array 데이터:', np.round(test_array.reshape(-1), 2))\n",
        "print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))"
      ],
      "metadata": {
        "id": "R0AjJk-iRZvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> 학습 데이터와 테스트 데이터가 모두 동일하게 변환되었음"
      ],
      "metadata": {
        "id": "w26MXsTBSY2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pg.131 : 사이킷런으로 타이타닉\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "titanic_df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/titanic_train.csv')\n",
        "titanic_df.head(3)"
      ],
      "metadata": {
        "id": "_RdSS-bfST6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 칼럼 타입 확인\n",
        "print('\\n ###학습 데이터 정보###\\n')\n",
        "print(titanic_df.info())    # 결과에서 RangeIndex : 데이터프레임 인덱스의 범위를 나타냄 / 전체 로우 수 파악가능"
      ],
      "metadata": {
        "id": "B1_iYteqUNLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Null값 처리 위한 값 유무 확인\n",
        "titanic_df['Age'].fillna(titanic_df['Age'].mean(),inplace=True) # 평균 나이로 변경\n",
        "titanic_df['Cabin'].fillna('N',inplace=True) # 'N'으로 변경\n",
        "titanic_df['Embarked'].fillna('N',inplace=True)\n",
        "print('데이터 세트 Null 값 개수',titanic_df.isnull().sum().sum)"
      ],
      "metadata": {
        "id": "OVuDM6TDUSNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 남아있는 문자열 피처 분류 살펴보기\n",
        "print('sex 값 분포:\\n',titanic_df['Sex'].value_counts())\n",
        "print('\\n Cabin 값 분포:\\n',titanic_df['Cabin'].value_counts())\n",
        "print('\\n Embarked 값 분포:\\n',titanic_df['Embarked'].value_counts())"
      ],
      "metadata": {
        "id": "L1cgs1TAUjj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cabin의 경우 N이 가장 많음 + 속성값이 제대로 정리되지 않음"
      ],
      "metadata": {
        "id": "ajaFB__7VZGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cabin 속성에서 앞 문자만 추출\n",
        "titanic_df['Cabin']=titanic_df['Cabin'].str[:1]\n",
        "print(titanic_df['Cabin'].head(3))"
      ],
      "metadata": {
        "id": "BhIVSh3FVLaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 성별에 따른 생존자 수\n",
        "titanic_df.groupby(['Sex','Survived'])['Survived'].count()"
      ],
      "metadata": {
        "id": "u-OLh6QDVnf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 성별에 따른 생존자 수 -> 시각화\n",
        "sns.barplot(x='Sex',y='Survived',data=titanic_df)"
      ],
      "metadata": {
        "id": "QwqgVdavVxXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 객실 등급별 성별에 따른 생존 확률 -> 시각화\n",
        "sns.barplot(x='Pclass',y='Survived',hue='Sex',data=titanic_df)"
      ],
      "metadata": {
        "id": "YLUhRBtqV_3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 age에 따라 구분 값을 반환하는 함수 설정, DataFrame의 apply lambda 식에 사용.\n",
        "def get_category(age):\n",
        "  cat=''\n",
        "  if age<=-1:cat='Unknown'\n",
        "  elif age<=5:cat='Baby'\n",
        "  elif age <=12:cat='Child'\n",
        "  elif age<=18:cat='Teenager'\n",
        "  elif age<=25:cat='Student'\n",
        "  elif age<=35:cat='Young Adult'\n",
        "  elif age<=60:cat='Adult'\n",
        "  else: cat='Elderly'\n",
        "\n",
        "  return cat"
      ],
      "metadata": {
        "id": "3uMydKVXWOwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#막대그래프의 크기 figure 를 더 크게 설정\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "#X축의 값을 순차적으로 표시하기 위한 설정\n",
        "group_names=['Unknown,','Baby','Child','Teenager','Student','Young Adult','Adult','Elderly']\n",
        "\n",
        "#lambda 식에 위에서 생성한 get_category() 함수를 반환값으로 지정\n",
        "#get_category(X)는 입력갑으로 'Age'칼럼 값을 받아서 해당하는 cat 반환\n",
        "titanic_df['Age_cat']=titanic_df['Age'].apply(lambda x:get_category(x))\n",
        "sns.barplot(x='Age_cat',y='Survived',hue='Sex',data=titanic_df,order=group_names)\n",
        "titanic_df.drop('Age_cat',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "e3Lpw7nCWWih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 남아있는 문자열 카테고리 피처를 숫자형 카테고리 피처로 변환\n",
        "from sklearn import preprocessing \n",
        "\n",
        "def encode_features(dataDF):    # 여러 칼럼을 한 번에 변환해주는 역할의 함수 생성\n",
        "  features=['Cabin','Sex','Embarked']\n",
        "  for feature in features:\n",
        "    le=preprocessing.LabelEncoder()   # 사이킷런의 LabelEncoder 클래스 이용해서 레이블 인코딩 적용용\n",
        "    le=le.fit(dataDF[feature])\n",
        "    dataDF[feature]=le.transform(dataDF[feature])\n",
        "  return dataDF\n",
        "\n",
        "titanic_df=encode_features(titanic_df)\n",
        "titanic_df.head()"
      ],
      "metadata": {
        "id": "iUtLpBmGWp0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 피처를 가공한 내역을 정리하고 이를 함수로 만들어 쉽게 재사용할 수 있도록 하기\n",
        "\n",
        "# Null 처리 함수\n",
        "def fillna(df):\n",
        "  df['Age'].fillna(df['Age'].mean(),inplace=True)\n",
        "  df['Cabin'].fillna('N',inplace=True)\n",
        "  df['Embarked'].fillna('N',inplace=True)\n",
        "  df['Fare'].fillna(0,inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "# 머신러닝 알고리즘에 불필요한 속성제거\n",
        "def drop_features(df):\n",
        "  df.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)\n",
        "  return df\n",
        "\n",
        "# 레이블 인코딩 수행\n",
        "def format_features(df):\n",
        "  df['Cabin']=df['Cabin'].str[:1]\n",
        "  features=['Cabin','Sex','Embarked']\n",
        "  for feature in features:\n",
        "    le=LabelEncoder()\n",
        "    le=le.fit(df[feature])\n",
        "    df[feature]=le.transform(df[feature])\n",
        "\n",
        "  return df\n",
        "\n",
        "#앞에서 설정한 데이터 전처리 함수 호출\n",
        "def transform_features(df):   # transform_features() : 데이터의 전처리를 전체적으로 호출하는 기능의 함수\n",
        "  df=fillna(df)\n",
        "  df=drop_features(df)\n",
        "  df=format_features(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "ovSaMAiTW6Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 원본 데이터를 재로딩하고, 피처 데이터 세트와 레이블 데이터 세트 추출\n",
        "titanic_df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/titanic_train.csv')\n",
        "y_titanic_df=titanic_df['Survived']   # Survived 속성만 별도 분리해서 클래스 결정값 데이터 세트로 만듦\n",
        "X_titanic_df=titanic_df.drop('Survived',axis=1)   # Survived 속성을 드롭해 피처 데이터 세트를 만듦\n",
        "\n",
        "X_titanic_df=transform_features(X_titanic_df)   # 데이터 가공"
      ],
      "metadata": {
        "id": "jaTd-thzYFBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_test_split() 이용해서 별도의 테스트 데이터 세트 추출\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X_titanic_df,y_titanic_df,test_size=0.2,random_state=11)"
      ],
      "metadata": {
        "id": "mRGsr-SoYoWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 결정트리, 랜덤포레스트, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성\n",
        "dt_clf=DecisionTreeClassifier(random_state=11)\n",
        "rf_clf=RandomForestClassifier(random_state=11)\n",
        "lr_clf=LogisticRegression()\n",
        "\n",
        "\n",
        "# DecisionTreeClassifier 학습/예측/평가\n",
        "dt_clf.fit(X_train,y_train)\n",
        "dt_pred=dt_clf.predict(X_test)\n",
        "print('DecisionTreecClassifier 정확도:{0:.4f}'.format(accuracy_score(y_test,dt_pred)))\n",
        "\n",
        "\n",
        "# RandomForestClassifier 학습/예측/평가\n",
        "rf_clf.fit(X_train,y_train)\n",
        "rf_pred=rf_clf.predict(X_test)\n",
        "print('RandomForestClassifier 정확도{0:.4f}'.format(accuracy_score(y_test,rf_pred)))\n",
        "\n",
        "# LogisticRegression 학습/예측/평가\n",
        "lr_clf.fit(X_train,y_train)\n",
        "lr_pred=lr_clf.predict(X_test)\n",
        "print('LogisticRegression 정확도{0:.4f}'.format(accuracy_score(y_test,lr_pred)))"
      ],
      "metadata": {
        "id": "dJQr6RiyZPkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 교차 검증으로 결정 트리 모델 좀 더 평가해보기 (KFold)\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def exec_kfold(clf, folds=5):\n",
        "  # 폴드 세트를 5개인 KFold객체를 생성, 폴드 수만큼 예측결과 저장을 위한 리스트 객체 생성\n",
        "  kfold = KFold(n_splits=folds)\n",
        "  scores=[]\n",
        "\n",
        "  # KFold 교차 검증 수행.\n",
        "  for iter_count, (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):\n",
        "\n",
        "    # X_titanic_df 데이터에서 교차 검증별로 학습과 검증 데이터를 가리키는 index 생성\n",
        "    X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]\n",
        "    y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]\n",
        "\n",
        "    # Classifier 학습, 예측, 정확도 계산\n",
        "    clf.fit(X_train, y_train)\n",
        "    predictions = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    scores.append(accuracy)\n",
        "    print(\"교차 검증 {0} 정확도: {1:.4f}\".format(iter_count, accuracy))\n",
        "\n",
        "  # 5개 fold에서의 평균 정확도 계산\n",
        "  mean_score = np.mean(scores)\n",
        "  print(\"평균 정확도: {0:.4f}\".format(mean_score))\n",
        "\n",
        "# exec_kfold 호출\n",
        "exec_kfold(dt_clf, folds=5)"
      ],
      "metadata": {
        "id": "jktIcrFhZotY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 교차 검증을 cross_val_score() 이용해서 수행하기\n",
        "# cross_val_score() : StratifiedKFold 이용해서 폴드 세트를 분할함\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(dt_clf, X_titanic_df, y_titanic_df, cv=5)\n",
        "for iter_count, accuracy in enumerate(scores):\n",
        "  print(\"교차 검증 {0} 정확도: {1:.4f}\".format(iter_count, accuracy))\n",
        "\n",
        "print(\"평균 정확도: {0:.4f}\".format(np.mean(scores)))"
      ],
      "metadata": {
        "id": "9ubkN_3BaYKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GridSearchCV 이용해서 DecisionTreeClassifier의 최적 하이퍼 파라미터 찾고 예측 성능 측정\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'max_depth':[2,3,5,10], 'min_samples_split':[2,3,5], 'min_samples_leaf':[1,5,8]}\n",
        "\n",
        "grid_dclf = GridSearchCV(dt_clf, param_grid=parameters, scoring='accuracy', cv=5)\n",
        "grid_dclf.fit(X_train, y_train)\n",
        "\n",
        "print('GridSearchCV 최적 하이퍼 파라미터 :', grid_dclf.best_params_)\n",
        "print('GirdSearchCV 최고 정확도: {0:.4f}'.format(grid_dclf.best_score_))\n",
        "best_dclf = grid_dclf.best_estimator_\n",
        "\n",
        "# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행\n",
        "dpredictions = best_dclf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, dpredictions)\n",
        "print('테스트 세트에서의 DecisionTreeClassifier 정확도 : {0:.4f}'.format(accuracy))"
      ],
      "metadata": {
        "id": "6DivFCrPaC_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C1IMsVp_alyw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}